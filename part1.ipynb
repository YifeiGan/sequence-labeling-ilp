{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1-gram model...\n",
      "1-gram Training Perplexity: 1129.0007325805495\n",
      "1-gram Development Perplexity: 1038.4668779311676\n",
      "1-gram Test Perplexity: 1037.8079658855668\n",
      "Training 2-gram model...\n",
      "2-gram Training Perplexity: 98.74207175896068\n",
      "2-gram Development Perplexity: inf\n",
      "2-gram Test Perplexity: inf\n",
      "Training 3-gram model...\n",
      "3-gram Training Perplexity: 8.058281666260303\n",
      "3-gram Development Perplexity: inf\n",
      "3-gram Test Perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.model = defaultdict(Counter)\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        tokenized_lines = []\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        # Preprocess each line and update counts\n",
    "        for line in corpus:\n",
    "            tokens = self.preprocess(line)\n",
    "            tokenized_lines.append(tokens)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Handle OOV by converting words that appear < 3 times to <UNK>\n",
    "        for word in word_counts:\n",
    "            if word_counts[word] < 3:\n",
    "                self.vocab.add('<UNK>')\n",
    "            else:\n",
    "                self.vocab.add(word)\n",
    "                \n",
    "        # Train n-gram model\n",
    "        for tokens in tokenized_lines:\n",
    "            tokens = ['<START>'] * (self.n - 1) + [token if token in self.vocab else '<UNK>' for token in tokens] + ['<STOP>']\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                context = tuple(tokens[i:i + self.n - 1])\n",
    "                word = tokens[i + self.n - 1]\n",
    "                self.model[context][word] += 1\n",
    "        \n",
    "    def get_ngram_prob(self, context, word):\n",
    "        context_count = sum(self.model[context].values())\n",
    "        word_count = self.model[context][word]\n",
    "        return word_count / context_count if context_count > 0 else 0.0\n",
    "\n",
    "    def calculate_perplexity(self, corpus):\n",
    "        total_log_prob = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for line in corpus:\n",
    "            tokens = ['<START>'] * (self.n - 1) + [token if token in self.vocab else '<UNK>' for token in self.preprocess(line)] + ['<STOP>']\n",
    "            total_tokens += len(tokens) - (self.n - 1)\n",
    "            \n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                context = tuple(tokens[i:i + self.n - 1])\n",
    "                word = tokens[i + self.n - 1]\n",
    "                prob = self.get_ngram_prob(context, word)\n",
    "                if prob > 0:\n",
    "                    total_log_prob += math.log(prob)\n",
    "                else:\n",
    "                    return float('inf')\n",
    "                    \n",
    "        avg_log_prob = total_log_prob / total_tokens\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        return perplexity\n",
    "\n",
    "# Read data files\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = '1b_benchmark.train.tokens'\n",
    "    dev_file = '1b_benchmark.dev.tokens'\n",
    "    test_file = '1b_benchmark.test.tokens'\n",
    "\n",
    "    # Load data\n",
    "    train_corpus = load_corpus(train_file)\n",
    "    dev_corpus = load_corpus(dev_file)\n",
    "    test_corpus = load_corpus(test_file)\n",
    "\n",
    "    # Train and evaluate models for unigram, bigram, and trigram\n",
    "    for n in [1, 2, 3]:\n",
    "        print(f\"Training {n}-gram model...\")\n",
    "        model = NgramLanguageModel(n)\n",
    "        model.train(train_corpus)\n",
    "\n",
    "        train_perplexity = model.calculate_perplexity(train_corpus)\n",
    "        dev_perplexity = model.calculate_perplexity(dev_corpus)\n",
    "        test_perplexity = model.calculate_perplexity(test_corpus)\n",
    "\n",
    "        print(f\"{n}-gram Training Perplexity: {train_perplexity}\")\n",
    "        print(f\"{n}-gram Development Perplexity: {dev_perplexity}\")\n",
    "        print(f\"{n}-gram Test Perplexity: {test_perplexity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
