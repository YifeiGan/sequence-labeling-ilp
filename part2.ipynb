{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1-gram model...\n",
      "Best lambdas for 1-gram: [1.]\n",
      "1-gram Training Perplexity: 1130.0970935401174\n",
      "1-gram Development Perplexity: 1040.7830165470816\n",
      "1-gram Test Perplexity: 1040.1657826290268\n",
      "Training 2-gram model...\n",
      "Best lambdas for 2-gram: [3.46725051e-14 1.00000000e+00]\n",
      "2-gram Training Perplexity: 2013.9531441572253\n",
      "2-gram Development Perplexity: 2429.310848371299\n",
      "2-gram Test Perplexity: 2414.680451657738\n",
      "Training 3-gram model...\n",
      "Best lambdas for 3-gram: [0. 0. 1.]\n",
      "3-gram Training Perplexity: 6750.764084596879\n",
      "3-gram Development Perplexity: 11054.39056358828\n",
      "3-gram Test Perplexity: 11004.951021273084\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    def __init__(self, n, lambdas=None):\n",
    "        self.n = n\n",
    "        self.model = defaultdict(Counter)\n",
    "        self.vocab = set()\n",
    "        if lambdas:\n",
    "            self.lambdas = lambdas\n",
    "        else:\n",
    "            self.lambdas = [1.0 / n] * n \n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        tokenized_lines = []\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        # Preprocess each line and update counts\n",
    "        for line in corpus:\n",
    "            tokens = self.preprocess(line)\n",
    "            tokenized_lines.append(tokens)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Handle OOV by converting words that appear < 3 times to <UNK>\n",
    "        for word in word_counts:\n",
    "            if word_counts[word] < 3:\n",
    "                self.vocab.add('<UNK>')\n",
    "            else:\n",
    "                self.vocab.add(word)\n",
    "                \n",
    "        # Train n-gram model\n",
    "        for tokens in tokenized_lines:\n",
    "            tokens = ['<START>'] * (self.n - 1) + [token if token in self.vocab else '<UNK>' for token in tokens] + ['<STOP>']\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                context = tuple(tokens[i:i + self.n - 1])\n",
    "                word = tokens[i + self.n - 1]\n",
    "                self.model[context][word] += 1\n",
    "        \n",
    "    def get_ngram_prob(self, context, word):\n",
    "        context_count = sum(self.model[context].values())\n",
    "        word_count = self.model[context][word]\n",
    "        vocab_size = len(self.vocab)\n",
    "        return (word_count + 1) / (context_count + vocab_size) if context_count > 0 else 1 / vocab_size  # Add-one smoothing with vocab size\n",
    "\n",
    "    def get_smoothed_prob(self, context, word):\n",
    "        prob = 0.0\n",
    "        for i in range(self.n):\n",
    "            sub_context = context[len(context) - i:]\n",
    "            prob += self.lambdas[i] * self.get_ngram_prob(sub_context, word)\n",
    "        return prob\n",
    "\n",
    "    def calculate_perplexity(self, corpus):\n",
    "        total_log_prob = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for line in corpus:\n",
    "            tokens = ['<START>'] * (self.n - 1) + [token if token in self.vocab else '<UNK>' for token in self.preprocess(line)] + ['<STOP>']\n",
    "            total_tokens += len(tokens) - (self.n - 1)\n",
    "            \n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                context = tuple(tokens[i:i + self.n - 1])\n",
    "                word = tokens[i + self.n - 1]\n",
    "                prob = self.get_smoothed_prob(context, word)\n",
    "                if prob > 0:\n",
    "                    total_log_prob += math.log(prob)\n",
    "                else:\n",
    "                    return float('inf')\n",
    "                    \n",
    "        avg_log_prob = total_log_prob / total_tokens\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        return perplexity\n",
    "\n",
    "# Read data files\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def find_best_lambdas(model, train_corpus, dev_corpus):\n",
    "    def objective_function(lambdas):\n",
    "        model.lambdas = lambdas\n",
    "        return model.calculate_perplexity(dev_corpus)\n",
    "    \n",
    "    # Initial lambda values for optimization\n",
    "    initial_lambdas = [1.0 / model.n] * model.n\n",
    "\n",
    "    # Constraints: lambdas must sum to 1 and be non-negative\n",
    "    constraints = ({'type': 'eq', 'fun': lambda lambdas: sum(lambdas) - 1})\n",
    "    bounds = [(0, 1) for _ in range(model.n)]\n",
    "\n",
    "    result = minimize(objective_function, initial_lambdas, bounds=bounds, constraints=constraints, method='SLSQP')\n",
    "    return result.x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = '1b_benchmark.train.tokens'\n",
    "    dev_file = '1b_benchmark.dev.tokens'\n",
    "    test_file = '1b_benchmark.test.tokens'\n",
    "\n",
    "    # Load data\n",
    "    train_corpus = load_corpus(train_file)\n",
    "    dev_corpus = load_corpus(dev_file)\n",
    "    test_corpus = load_corpus(test_file)\n",
    "\n",
    "    # Train and evaluate models for unigram, bigram, and trigram\n",
    "    for n in [1, 2, 3]:\n",
    "        print(f\"Training {n}-gram model...\")\n",
    "        model = NgramLanguageModel(n)\n",
    "        model.train(train_corpus)\n",
    "\n",
    "        # Find the best lambda values for linear interpolation\n",
    "        best_lambdas = find_best_lambdas(model, train_corpus, dev_corpus)\n",
    "        model.lambdas = best_lambdas\n",
    "        print(f\"Best lambdas for {n}-gram: {best_lambdas}\")\n",
    "\n",
    "        train_perplexity = model.calculate_perplexity(train_corpus)\n",
    "        dev_perplexity = model.calculate_perplexity(dev_corpus)\n",
    "        test_perplexity = model.calculate_perplexity(test_corpus)\n",
    "\n",
    "        print(f\"{n}-gram Training Perplexity: {train_perplexity}\")\n",
    "        print(f\"{n}-gram Development Perplexity: {dev_perplexity}\")\n",
    "        print(f\"{n}-gram Test Perplexity: {test_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
